{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fullpth = os.path.abspath(os.path.curdir)\n",
    "while os.path.basename(os.path.abspath(os.path.curdir)) != 'multi-instance-mask-rcnn-extension':\n",
    "    %cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "traindir_roots = glob('/home/adelgior/code/multi-instance-mask-rcnn-extension/output/logs/test/train_2021-06*')\n",
    "traindir_roots += glob('/home/adelgior/code/multi-instance-mask-rcnn-extension/output/logs/test/train_2021-05*')\n",
    "# traindir_roots = glob('/home/adelgior/afs_directories/espresso/' \\\n",
    "#                       'code/multi-instance-mask-rcnn-extension/output/logs/test/train_2021-06*')\n",
    "# traindir_roots.append('/home/adelgior/afs_directories/espresso/code/multi-instance-mask-rcnn-extension/output/logs/test/dummytest')\n",
    "\n",
    "def decompose_cocoeval_file(filepath):\n",
    "    assert os.path.exists(filepath), filepath + ' does not exist'\n",
    "    print(f\"Loading {filepath}\")\n",
    "    d = torch.load(filepath)\n",
    "    decomp_dir = filepath.replace('.pth', '-decomp/')\n",
    "    if os.path.exists(decomp_dir):\n",
    "        print(f\"Warning: directory already exists.\")\n",
    "    else:\n",
    "        os.makedirs(decomp_dir)\n",
    "    for taskname in d.keys():\n",
    "        outname = os.path.join(decomp_dir, '-'.join(x for x in taskname) + '.pth')\n",
    "        print(f\"Saving {outname}\")\n",
    "        torch.save(d[taskname], outname)\n",
    "\n",
    "traindirs = {}\n",
    "for d in traindir_roots:\n",
    "    lst = sorted(glob(os.path.join(d, 'coco_2017_val', 'itr*')))\n",
    "#     lst = [lst[-1]]  # keep only the last iteration\n",
    "\n",
    "    iterations = [int(os.path.basename(l).replace('itr', '')) for l in lst]\n",
    "    last_saved = np.argmax(iterations)\n",
    "    if len(iterations) == 0:\n",
    "        continue\n",
    "    lst = [lst[last_saved]]\n",
    "    iterations = [iterations[last_saved]]\n",
    "    if max(iterations) == 0:\n",
    "        continue\n",
    "    for i, l in zip(iterations, lst):\n",
    "        if not os.path.exists(os.path.join(l, 'cocoevals-decomp/')):\n",
    "            print('Decomposing the cocoeval file')\n",
    "            decompose_cocoeval_file(os.path.join(l, 'cocoevals.pth'))\n",
    "        task_segm_files = glob(os.path.join(l, 'cocoevals-decomp/*-segm.pth'))\n",
    "        tasks = [os.path.splitext(os.path.basename(t))[0] for t in task_segm_files]\n",
    "        traindirs[(os.path.basename(d), i)] = {t: f for t, f in zip(tasks, task_segm_files)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compile stats by category; store imageids in a separate indexing list\n",
    "\n",
    "def get_df_agg_from_cocoeval(evalImgs, params):\n",
    "    N = len(params.imgIds)  # number of images\n",
    "    C = len(params.catIds)\n",
    "    A = len(params.areaRng)\n",
    "\n",
    "    def getimcatareaid(imidx, areaidx, catidx):\n",
    "        return A*N*k + N*j + i\n",
    "\n",
    "    agg = {'ious_bygt': {},\n",
    "           'gtids': {},\n",
    "           'dt_fp': {},\n",
    "           'gt_missed': {}\n",
    "          }\n",
    "    imcatarea_df_fp = []\n",
    "    imcatarea_df_fn = []\n",
    "    imcatarea_df_tp = []\n",
    "    imcatarea_imgId = []\n",
    "    imcatarea_catId = []\n",
    "    imcatarea_ninst = []\n",
    "    imcatarea_areaRng = []\n",
    "    inst_df_iou = []\n",
    "    inst_df_gtid = []\n",
    "    inst_df_catId = []\n",
    "    inst_df_imgId = []\n",
    "    inst_df_areaRng = []\n",
    "    for k, catid in enumerate(params.catIds):\n",
    "        for j, arng in enumerate(params.areaRng[:1]):\n",
    "            for i, imgid in enumerate(params.imgIds):\n",
    "                imcatareaidx = getimcatareaid(i, j, k)\n",
    "#                 print(i, j, k, imcatareaidx)\n",
    "                if evalImgs[imcatareaidx] is not None:\n",
    "                    n_inst = len(evalImgs[imcatareaidx]['gtIds'])\n",
    "                    assert (evalImgs[imcatareaidx]['image_id'], evalImgs[imcatareaidx]['category_id'], evalImgs[imcatareaidx]['aRng']) == (imgid, catid, arng)\n",
    "                    inst_df_iou.extend(evalImgs[imcatareaidx]['gtMatchIous'][0, :])\n",
    "                    inst_df_gtid.extend(evalImgs[imcatareaidx]['gtIds'])\n",
    "                    inst_df_catId.extend([catid for _ in range(n_inst)])\n",
    "                    inst_df_imgId.extend([imgid for _ in range(n_inst)])\n",
    "                    inst_df_areaRng.extend([arng for _ in range(n_inst)])\n",
    "                    fp = sum(x == 0 for x in evalImgs[imcatareaidx]['dtMatches'][0, :])\n",
    "                    tp = sum(x != 0 for x in evalImgs[imcatareaidx]['gtMatches'][0, :])\n",
    "                    fn = sum(x == 0 for x in evalImgs[imcatareaidx]['gtMatches'][0, :])\n",
    "                    ninst = len(evalImgs[imcatareaidx]['gtIds'])\n",
    "                else:\n",
    "                    fp, tp, fn = -1, -1, -1\n",
    "                    ninst = 0\n",
    "                imcatarea_df_fp.append(fp)\n",
    "                imcatarea_df_fn.append(fn)\n",
    "                imcatarea_df_tp.append(tp)\n",
    "                imcatarea_imgId.append(imgid)\n",
    "                imcatarea_catId.append(catid)\n",
    "                imcatarea_areaRng.append(arng)\n",
    "                imcatarea_ninst.append(ninst)\n",
    "\n",
    "\n",
    "    inst_df = pd.DataFrame(\n",
    "        data={\n",
    "        'iou': inst_df_iou, \n",
    "        'gtid': inst_df_gtid,\n",
    "        'category_id': inst_df_catId,\n",
    "        'image_id': inst_df_imgId,\n",
    "        'aRng': inst_df_areaRng\n",
    "    }, index=None)\n",
    "    imcatarea_df = pd.DataFrame(\n",
    "        data={\n",
    "            'fp': imcatarea_df_fp,\n",
    "            'tp': imcatarea_df_tp,\n",
    "            'fn': imcatarea_df_fn,\n",
    "            'image_id': imcatarea_imgId,\n",
    "            'n_inst': imcatarea_ninst,\n",
    "        }, index=None)\n",
    "    return inst_df, imcatarea_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load cocoevals and convert to dataframes if not cached.\n",
    "\n",
    "cocoevals = {}\n",
    "i = 0\n",
    "df_csv_names = {}\n",
    "for k, v in traindirs.items():\n",
    "    i += 1\n",
    "    print(f\"{i}/{len(traindirs)}\")\n",
    "    cocoeval = {}\n",
    "    for k1, v1 in v.items():\n",
    "        inst_df_csv_name = v1.replace('.pth', '-df-inst.csv')\n",
    "        imcatarea_df_csv_name = v1.replace('.pth', '-df-imcatarea.csv')\n",
    "        df_csv_names[(k, k1)] = {'inst': inst_df_csv_name, 'imcatarea': imcatarea_df_csv_name}\n",
    "\n",
    "inst_dfs = {}\n",
    "imcatarea_dfs = {}\n",
    "bad_keys = []\n",
    "i = 0\n",
    "for (k, k1), csv_names in df_csv_names.items():\n",
    "    i += 1\n",
    "    if not all(os.path.exists(v) for v in csv_names.values()):\n",
    "        print(f\"loading {i}/{len(csv_names)} from {traindirs[k][k1]}\")\n",
    "        try:\n",
    "            cocoeval = torch.load(traindirs[k][k1])\n",
    "        except Exception as ex:\n",
    "            bad_keys.append((k, k1))\n",
    "            print(f\"FAILED Loading from {traindirs[k][k1]}\")\n",
    "            print(ex)\n",
    "            continue\n",
    "        inst_df_, imcatarea_df_ = get_df_agg_from_cocoeval(cocoeval.evalImgs, cocoeval.params)\n",
    "        print(inst_df_)\n",
    "        inst_df_.to_csv(csv_names['inst'])\n",
    "        imcatarea_df_.to_csv(csv_names['imcatarea'])\n",
    "    else:\n",
    "        print('Files already exist:')\n",
    "        for v in csv_names.values():\n",
    "            print(v)    \n",
    "    i += 1\n",
    "for (k, k1) in bad_keys:\n",
    "    del df_csv_names[(k, k1)]\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inst_dfs = {}\n",
    "imcatarea_dfs = {}\n",
    "for (k, k1), csv_names in df_csv_names.items():\n",
    "    print(f\"Loading ({k}, {k1})\")\n",
    "    inst_df = pd.read_csv(csv_names['inst'])\n",
    "    imcatarea_df = pd.read_csv(csv_names['imcatarea'])\n",
    "    inst_dfs[(k, k1)] = inst_df\n",
    "    imcatarea_dfs[(k, k1)] = imcatarea_df\n",
    "\n",
    "NA_column = 'Unnamed: 0'\n",
    "for k_ in imcatarea_dfs.keys():\n",
    "    if NA_column in imcatarea_dfs[k_].columns:\n",
    "        imcatarea_dfs[k_] = imcatarea_dfs[k_].drop(NA_column, axis=1)\n",
    "    if NA_column in inst_dfs[k_].columns:\n",
    "        inst_dfs[k_] = inst_dfs[k_].drop(NA_column, axis=1)\n",
    "\n",
    "n_inst = None\n",
    "for k, df_ in inst_dfs.items():\n",
    "    if n_inst is None:\n",
    "        n_inst = len(df_)\n",
    "    else:\n",
    "        assert n_inst == len(df_)\n",
    "    if 'model_type' in df_:\n",
    "        df_ = df_.drop('model_type', axis=1)\n",
    "    df_.insert(1, 'model_type', k[0][0])\n",
    "    if 'itr' in df_:\n",
    "        df_ = df_.drop('itr', axis=1)\n",
    "    df_.insert(2, 'itr', k[0][1])\n",
    "    if 'task' in df_:\n",
    "        df_ = df_.drop('task', axis=1)\n",
    "    df_.insert(3, 'task', k[1])\n",
    "    if 'full_exp_id' in df_:\n",
    "        df_ = df_.drop('full_exp_id', axis=1)\n",
    "    df_.insert(len(df_.columns), 'full_exp_id', f\"{k}\")\n",
    "#     df_['exp_id'] = k\n",
    "\n",
    "inst_df_all = pd.concat(inst_dfs.values())\n",
    "\n",
    "full_exp_ids = inst_df_all.full_exp_id.unique()\n",
    "for exp_id in full_exp_ids:\n",
    "    assert sum(inst_df_all.full_exp_id == exp_id) == n_inst\n",
    "print(f\"{n_inst} per val, as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdirs = {}\n",
    "for k in df_csv_names.keys():\n",
    "    logdirs[k] = df_csv_names[k]['inst'].split('cocoevals-decomp')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pdanalysis\n",
    "import pandas as pd\n",
    "# from multimaskextension.analysis.pdanalysis import get_datetime, findDiff, get_value_from_cfg, \\\n",
    "#     logdirs_to_df, inplace_augment_df_with_cfg_columns\n",
    "from multimaskextension.analysis import pdanalysis\n",
    "df = pd.DataFrame(dict(logdir=logdirs.values(), config_file=[os.path.join(x, 'config_resume.yaml') for x in logdirs.values()]))\n",
    "pdanalysis.inplace_augment_df_with_cfg_columns(df, auto_diffcfg_columns=True, manual_cfg_allowdict=None,\n",
    "                                        cf_name='config_file', blocklist=(('DATALOADER', 'SEED')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?inplace_augment_df_with_cfg_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logdir_base'] = [os.path.basename(os.path.dirname(os.path.dirname(x.rstrip('/')))) for x in df.logdir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp = df[df['DATALOADER-SAMPLER_TRAIN'] == 'GeneralizedRepeatFactorTrainingSampler']\n",
    "dftmp = dftmp[dftmp['MODEL-ROI_HEADS-NAME'] == 'MultiROIHeadsAPD']\n",
    "# dftmp = dftmp[dftmp['MODEL-ROI_MASK_HEAD-INIT_ACTIVATED_MASK_HEAD'] == 'custom']\n",
    "# dftmp = dftmp[dftmp['MODEL-ROI_MASK_HEAD-MATCHING_LOSS'] == False]\n",
    "dftmp = dftmp[dftmp['DATALOADER-REPEAT_THRESHOLD'] == 0.01]\n",
    "dftmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in inst_dfs.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(inst_dfs.keys())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uniform w/ DATALOADER seed\n",
    "# k1 = (('train_2021-06-19-172356_VCS-3b1367f_MAX_ITR-100000_HEAD_TYPE-None', 64000), 'pred_masks-segm')\n",
    "# k2 = (('train_2021-06-19-011917_VCS-776a559_MAX_ITR-100000_HEAD_TYPE-custom_MATCH-0', 64000), 'pred_masks1-segm')\n",
    "\n",
    "# Non-uniform\n",
    "k1 = (('train_2021-06-19-172035_VCS-812882b_MAX_ITR-100000_HEAD_TYPE-standard', 64000), 'pred_masks-segm')\n",
    "# k2 = (('train_2021-05-04-104033_VCS-af9d7dd_MAX_ITR-100000_HEAD_TYPE-custom_MATCH-0', 64000), 'pred_masks1-segm')\n",
    "k2 = (('train_2021-06-20-184444_VCS-3b1367f_MAX_ITR-100000_HEAD_TYPE-custom_MATCH-0', 36000), 'pred_masks1-segm')\n",
    "# k2 = (('train_2021-05-04-103840_VCS-af9d7dd_MAX_ITR-100000_HEAD_TYPE-custom_MATCH-1', 64000), 'pred_masks1-segm')\n",
    "\n",
    "imcatarea_dfs[k1]\n",
    "imcatarea_dfs[k2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1_config = df[df.logdir_base == k1[0][0]]\n",
    "k2_config = df[df.logdir_base == k2[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([k1_config, k2_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "expid = k1\n",
    "x = imcatarea_dfs[expid].fp\n",
    "plt.hist(x[x!= -1], bins=np.arange(-1,10, 1)); plt.title('fp per im-cat'); plt.show()\n",
    "x = imcatarea_dfs[expid].tp\n",
    "plt.hist(x[x!= -1], bins=np.arange(-1,10, 1)); plt.title('tp per im-cat'); plt.show()\n",
    "x = imcatarea_dfs[expid].fn\n",
    "plt.hist(x[x!= -1], bins=np.arange(-1,10, 1)); plt.title('fn per im-cat'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_id1 = (('HEAD_TYPE-None', 36000), 'pred_masks-segm')\n",
    "# exp_id1 = (('HEAD_TYPE-None', 0), 'agg-pred_masks-segm')\n",
    "# exp_id1 = (('dummy', 0), 'pred_masks-segm')\n",
    "# exp_id2 = (('HEAD_TYPE-custom_MATCH-1', 36000), 'pred_masks-segm')\n",
    "exp_id1 = k1\n",
    "exp_id2 = k2\n",
    "\n",
    "print(f\"iou1: {exp_id1}\")\n",
    "print(f\"iou2: {exp_id2}\\n\")\n",
    "\n",
    "def subtract_ious(dfa, dfb):  # dfa.iou - dfb.iou with assertions\n",
    "    assert all(dfa.gtid == dfb.gtid)\n",
    "    assert len(dfa) == len(set(dfa.gtid)), 'GT IDs are not unique'\n",
    "    return dfa.iou - dfb.iou\n",
    "\n",
    "df1 = inst_df_all.loc[inst_df_all.full_exp_id == f\"{exp_id1}\"]\n",
    "df2 = inst_df_all.loc[inst_df_all.full_exp_id == f\"{exp_id2}\"]\n",
    "assert len(df1) > 0\n",
    "diff = subtract_ious(df1, df2)  # runs assertions before subtracting\n",
    "\n",
    "print(\"========= Soft score differences ==========\")\n",
    "print('IOU differences (iou1 - iou2):')\n",
    "print('     Mean,            min,             max')\n",
    "print(diff.mean(), diff.min(), diff.max())\n",
    "\n",
    "print(f\"iou1 and iou2 both had iou=0 for {((df1.iou == df2.iou) & (df2.iou == 0)).sum()} ({100 * ((df1.iou == df2.iou) & (df2.iou == 0)).sum()/len(df1):.2f}%) instances (iou1=iou2=0)\")\n",
    "print(f\"iou1 and iou2 both had the same nonzero iou for {((df1.iou == df2.iou) & (df2.iou != 0)).sum()} instances ({100 * ((df1.iou == df2.iou) & (df2.iou != 0)).sum()/len(df1):.2f}%) (iou1=iou2!=0)\")\n",
    "print(f\"iou1 > iou2 for {(df1.iou > df2.iou).sum()} ({100 * (df1.iou > df2.iou).sum()/len(df2):.2f}%) instances\")\n",
    "print(f\"iou2 > iou1 for {(df2.iou > df1.iou).sum()} ({100 * (df2.iou > df1.iou).sum()/len(df1):.2f}%) instances\")\n",
    "\n",
    "iou1_missed = df1.iou == 0\n",
    "iou2_missed = df2.iou == 0\n",
    "\n",
    "iou1_found = df1.iou >= 0.5\n",
    "iou2_found = df2.iou >= 0.5\n",
    "\n",
    "assert len(df1.iou) == sum(iou1_missed) + sum(iou1_found)\n",
    "assert len(df2.iou) == sum(iou2_missed) + sum(iou2_found)\n",
    "assert all(df1.full_exp_id == f\"{exp_id1}\")\n",
    "assert all(df2.full_exp_id == f\"{exp_id2}\")\n",
    "\n",
    "print(\"(The following has passed every assertion statement I could think of):\\n\")\n",
    "\n",
    "# Useful stats\n",
    "\n",
    "print(\"========= Hard score differences ==========\")\n",
    "print(f\"iou1 missed {sum(iou1_missed)} instances ({100*sum(iou1_missed)/len(iou1_missed):.2f}% of total instances)\")\n",
    "print(f\"iou2 missed {sum(iou2_missed)} instances ({100*sum(iou2_missed)/len(iou2_missed):.2f}% of total instances)\")\n",
    "print(f\"iou1 found* {sum(iou1_found & iou2_missed)} instances of the {sum(iou2_missed)} that iou2 missed ({100 * sum(iou1_found & iou2_missed) / sum(iou2_missed):.2f}%)\")\n",
    "print(f\"iou2 found* {sum(iou2_found & iou1_missed)} instances of the {sum(iou1_missed)} that iou1 missed ({100 * sum(iou2_found & iou1_missed) / sum(iou1_missed):.2f}%)\")\n",
    "\n",
    "print(f\"\\n======== Summary ===========\")\n",
    "\n",
    "better = \"iou1\" if (df1.iou > df2.iou).sum() > (df2.iou > df1.iou).sum() else \"iou2\"\n",
    "print(f\"It appears {better} is better according to soft scores (direct per-instance IOU comparisons)\")\n",
    "print(f\"\\t- Better IOU on {100 * max((df2.iou > df1.iou).sum(), (df1.iou > df2.iou).sum())/len(df2):.2f}% vs {100 * min((df2.iou > df1.iou).sum(), (df1.iou > df2.iou).sum())/len(df2):.2f}% of instances \")\n",
    "better = \"iou1\" if sum(iou1_found) >= sum(iou2_found) else \"iou2\"\n",
    "worse = \"iou2\" if better == \"iou1\" else \"iou1\"\n",
    "print(f\"\\nIt appears {better} is better according to hard scores (total recall -- TP/gtP ).\")\n",
    "better_recall = sum(iou1_found)/len(df1) if better == \"iou1\" else sum(iou2_found)/len(df1)\n",
    "worse_recall = sum(iou2_found)/len(df1) if worse == \"iou2\" else sum(iou1_found)/len(df1)\n",
    "print(f\"\\t- Recall: {100 * better_recall:.2f} vs {100 * worse_recall:.2f}\")\n",
    "iou1_imp = sum(iou1_found & iou2_missed) / sum(iou2_missed)\n",
    "iou2_imp = sum(iou2_found & iou1_missed) / sum(iou1_missed)\n",
    "better_imp = iou1_imp if better == \"iou1\" else iou2_imp\n",
    "worse_imp = iou2_imp if worse == \"iou2\" else iou1_imp\n",
    "print(f\"\\t- Found {100 * better_imp:.2f}% of the instances {worse} missed vs {100 * worse_imp:.2f}% vice versa\")\n",
    "\n",
    "print(f\"\\n*note: 'found' means IOU>=0.5 (fairly strict)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byimid = df1[iou2_found & iou1_missed].groupby('image_id')\n",
    "image_ids = list(byimid.groups)\n",
    "n_found = [len(byimid.get_group(imid)) for imid in image_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{image_ids[x]: n_found[x] for x in np.argsort(n_found)[::-1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_sorted = df1.loc[diff.argsort().values, :]\n",
    "df2_sorted = df2.loc[diff.argsort().values, :]\n",
    "(df1_sorted.image_id== df2_sorted.image_id).all()\n",
    "\n",
    "comparison_df1_2 = pd.DataFrame(\n",
    "    {'iou2': df2_sorted['iou'],\n",
    "     'iou1': df1_sorted['iou'],\n",
    "     'category_id': df1_sorted['category_id'],\n",
    "     'image_id': df1_sorted['image_id']\n",
    "    }\n",
    ")\n",
    "\n",
    "comparison_by_imageid = comparison_df1_2.groupby('image_id')\n",
    "\n",
    "image_ids = list(comparison_by_imageid.groups)\n",
    "img_agg = pd.DataFrame({'image_id': image_ids})\n",
    "img_agg['max_1v2'] = 0\n",
    "img_agg['min_1v2'] = 0\n",
    "\n",
    "im_id_diffs = []\n",
    "for i, im_id in enumerate(image_ids):\n",
    "    compdf = comparison_by_imageid.get_group(im_id)\n",
    "    diff1v2 = compdf.iou1 - compdf.iou2\n",
    "    img_agg.loc[i, 'max_1v2'] = diff1v2.max()\n",
    "    img_agg.loc[i, 'min_1v2'] = diff1v2.min()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = os.path.expanduser('/tmp/sorting/')\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "img_agg = img_agg.sort_values('min_1v2')\n",
    "img_agg.image_id.to_csv(os.path.join(outdir, \"sorted_image_id_by_min_1v2.txt\"), sep='\\n', index=False)\n",
    "img_agg = img_agg.sort_values('max_1v2')\n",
    "img_agg.image_id.to_csv(os.path.join(outdir, \"sorted_image_id_by_max_1v2.txt\"), index=False)\n",
    "with open(os.path.join(outdir, '1_2_ids.txt'), 'w') as f:\n",
    "    f.write(f\"1: {exp_id1}\")\n",
    "    f.write(f\"2: {exp_id2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(img_agg.min_1v2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(img_agg.max_1v2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(diff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Histogram class for bokeh\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.palettes import Spectral5, Turbo256\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "import itertools\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "from bokeh.models import FactorRange\n",
    "from bokeh.layouts import gridplot\n",
    "import bokeh_catplot\n",
    "import colorcet as cc\n",
    "\n",
    "\n",
    "palette = [cc.rainbow[i*15] for i in range(17)]\n",
    "\n",
    "class BokehHistogram():\n",
    "    default_bins = 30\n",
    "    def __init__(self, colors=palette, height=600, width=600):\n",
    "        self.colors = colors\n",
    "        self.alpha = 0.5\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "    def empty_figure(self):\n",
    "        return figure(plot_height=self.height, plot_width=self.width)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_np_hist(data, bins=None):\n",
    "        bins = bins if bins is not None else self.default_bins\n",
    "        hist, edges = np.histogram(data, bins=bins)\n",
    "        return hist, edges\n",
    "\n",
    "    def hist_hover(self, dataframe, column, bins=30, log_scale=False, show_plot=False, title=None):\n",
    "        bins = bins or self.default_bins\n",
    "        hist, edges = self.get_np_hist(dataframe[column], bins=bins)\n",
    "        hist_df = pd.DataFrame({column: hist,\n",
    "                                 \"left\": edges[:-1],\n",
    "                                 \"right\": edges[1:]})\n",
    "        hist_df[\"interval\"] = [\"%d to %d\" % (left, right) for left, \n",
    "                               right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n",
    "\n",
    "        if log_scale == True:\n",
    "            hist_df[\"log\"] = np.log(hist_df[column])\n",
    "            src = ColumnDataSource(hist_df)\n",
    "            plot = figure(plot_height = self.height, plot_width = self.width,\n",
    "                  title = title or \"Histogram of {}\".format(column.capitalize()),\n",
    "                  x_axis_label = column.capitalize(),\n",
    "                  y_axis_label = \"Log Count\")    \n",
    "            plot.quad(bottom = 0, top = \"log\",left = \"left\", \n",
    "                right = \"right\", source = src, fill_color = self.colors[0], \n",
    "                line_color = \"black\", fill_alpha = alpha,\n",
    "                hover_fill_alpha = 1.0, hover_fill_color = self.colors[1])\n",
    "        else:\n",
    "            src = ColumnDataSource(hist_df)\n",
    "            plot = figure(plot_height = self.height, plot_width = self.width,\n",
    "                  title = title or \"Histogram of {}\".format(column.capitalize()),\n",
    "                  x_axis_label = column.capitalize(),\n",
    "                  y_axis_label = \"Count\")\n",
    "            plot.quad(bottom = 0, top = column,left = \"left\", \n",
    "                right = \"right\", source = src, fill_color = self.colors[0], \n",
    "                line_color = \"black\", fill_alpha = 0.7,\n",
    "                hover_fill_alpha = 1.0, hover_fill_color = self.colors[1])\n",
    "\n",
    "        hover = HoverTool(tooltips = [('Interval', '@interval'),\n",
    "                                  ('Count', str(\"@\" + column))])\n",
    "        plot.add_tools(hover)\n",
    "\n",
    "        if show_plot == True:\n",
    "            show(plot)\n",
    "        return plot\n",
    "\n",
    "    def histotabs(self, dataframe, features, log_scale=False, show_plot=False):\n",
    "        hists = []\n",
    "        for f in features:\n",
    "            h = self.hist_hover(dataframe, f, log_scale=log_scale, show_plot=show_plot)\n",
    "            p = Panel(child=h, title=f.capitalize())\n",
    "            hists.append(p)\n",
    "        t = Tabs(tabs=hists)\n",
    "        if show_plot:\n",
    "            show(t)\n",
    "        return t\n",
    "\n",
    "    def filtered_histotabs(self, dataframe, feature, filter_feature, log_scale=False, show_plot=False, title=None, sync_x=True, sync_y=True, xlim=None, ylim=None):\n",
    "        hists = []\n",
    "        x_min, x_max, y_min, y_max = 1e9, -1e9, 1e9, -1e9\n",
    "        for col in dataframe[filter_feature].unique():\n",
    "            sub_df = dataframe[dataframe[filter_feature] == col]\n",
    "            histo = self.hist_hover(sub_df, feature, log_scale=log_scale, show_plot=show_plot)\n",
    "            if sync_y is True and ylim is None:\n",
    "                hist_data, edges = self.get_np_hist(sub_df[feature], bins=self.default_bins)\n",
    "                y_min, y_max = min(y_min, min(hist_data)), max(y_max, max(hist_data))\n",
    "            if sync_x is True and xlim is None:\n",
    "                x_min, x_max = min(x_min, min(sub_df[feature])), max(x_max, max(sub_df[feature]))\n",
    "            p = Panel(child = histo, title=title or col)\n",
    "            hists.append(p)\n",
    "        if sync_y:\n",
    "            if ylim is not None:\n",
    "                y_min, y_max = ylim\n",
    "            for h in hists:\n",
    "                h.child.y_range=Range1d(y_min, y_max)\n",
    "        if sync_x:\n",
    "            if xlim is not None:\n",
    "                x_min, x_max = xlim\n",
    "            for h in hists:\n",
    "                h.child.x_range=Range1d(x_min, x_max)\n",
    "\n",
    "        t = Tabs(tabs=hists)\n",
    "        if show_plot:\n",
    "            show(t)\n",
    "        return t\n",
    "\n",
    "    \n",
    "    def filtered_stacked_hist(self, dataframe, feature, filter_feature, log_scale=False, show_plot=False, sync_x=True, sync_y=True, xlim=None, ylim=None):\n",
    "        hists = []\n",
    "        x_min, x_max, y_min, y_max = 1e9, -1e9, 1e9, -1e9\n",
    "        for col in dataframe[filter_feature].unique():\n",
    "            sub_df = dataframe[dataframe[filter_feature] == col]\n",
    "            histo = self.hist_hover(sub_df, feature, log_scale=log_scale, show_plot=show_plot, title=col)\n",
    "            if sync_y is True and ylim is None:\n",
    "                hist_data, edges = self.get_np_hist(sub_df[feature], bins=self.default_bins)\n",
    "                y_min, y_max = min(y_min, min(hist_data)), max(y_max, max(hist_data))\n",
    "            if sync_x is True and xlim is None:\n",
    "                x_min, x_max = min(x_min, min(sub_df[feature])), max(x_max, max(sub_df[feature]))\n",
    "            hists.append(histo)\n",
    "            \n",
    "        if sync_y:\n",
    "            if ylim is not None:\n",
    "                y_min, y_max = ylim\n",
    "            for h in hists:\n",
    "                h.y_range=Range1d(y_min, y_max)\n",
    "        if sync_x:\n",
    "            if xlim is not None:\n",
    "                x_min, x_max = xlim\n",
    "            for h in hists:\n",
    "                h.x_range=Range1d(x_min, x_max)\n",
    "            \n",
    "        layout = gridplot([[h] for h in hists])\n",
    "        t = layout\n",
    "        if show_plot:\n",
    "            show(t)\n",
    "        return t\n",
    "\n",
    "    def filtered_stacked_catplots(self, dataframe, feature, filter_feature, show_plot=False):\n",
    "        t = bokeh_catplot.box(\n",
    "            data=dataframe,\n",
    "            cats=filter_feature,\n",
    "            val=feature,\n",
    "            plot_height = self.height, plot_width = self.width\n",
    "        )\n",
    "        if show_plot:\n",
    "            show(t)\n",
    "        return t\n",
    "\n",
    "    def catplotlib_histogram(self, dataframe, feature, filter_feature, show_plot=False):\n",
    "        t = bokeh_catplot.histogram(\n",
    "            data=dataframe,\n",
    "            cats=filter_feature,\n",
    "            val=feature,\n",
    "            plot_height = self.height, plot_width = self.width\n",
    "        )\n",
    "        if show_plot:\n",
    "            show(t)\n",
    "        return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.layouts import column, row  # for suptitle\n",
    "from bokeh.models import Div      # for suptitle\n",
    "output_file(\"filename.html\")\n",
    "\n",
    "def mask_name_order(mask_name):\n",
    "    if 'agg' in mask_name:\n",
    "        return 3\n",
    "    elif 'pred_mask1' in mask_name:\n",
    "        return 2\n",
    "    elif 'pred_mask2' in mask_name:\n",
    "        return 1\n",
    "    elif 'pred_mask' in mask_name:\n",
    "        return 10\n",
    "    else:\n",
    "        return 100\n",
    "    \n",
    "h = BokehHistogram()\n",
    "h.height = 200\n",
    "h.width = 800\n",
    "\n",
    "for sync in [True, False]:\n",
    "    xsync = ysync = sync\n",
    "    title = Div(text=f\"<h2>IOU distribution: each plot is a different trained model's mask (axes_sync={sync})</h2>\", width=h.width, height=h.height // 2)\n",
    "    xl = None\n",
    "    if sync:\n",
    "        n_gt_instances = (inst_df_all.full_exp_id == list(inst_df_all.full_exp_id)[0]).sum()\n",
    "        yl = (0, n_gt_instances)\n",
    "    else:\n",
    "        yl = None\n",
    "    multimask_hists = []\n",
    "    for model_type in inst_df_all.model_type.unique():\n",
    "        for itr in inst_df_all.itr.unique():\n",
    "            sort_fcn = lambda xvec: [mask_name_order(x) for x in xvec]\n",
    "            dfs = inst_df_all.loc[(inst_df_all.model_type == model_type) & (inst_df_all.itr == itr), :].sort_values(by='task', inplace=False, key=sort_fcn)\n",
    "            if len(dfs) == 0:\n",
    "                multimask_hists.append(h.empty_figure())\n",
    "            else:\n",
    "                multimask_hists.append(h.filtered_stacked_hist(dfs, 'iou', 'full_exp_id', log_scale=False, sync_x=xsync, sync_y=ysync, ylim=yl, xlim=xl))\n",
    "    show(column(title, row(multimask_hists)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_full_exp_id_strs(full_exp_id_str):\n",
    "    \n",
    "df_for_sorting_expids = inst_df_all.loc[:, ['full_exp_id', 'model_type', 'itr', 'task']]\n",
    "df_for_sorting_expids = df_for_sorting_expids.sort_values('task', key=lambda xvec: [mask_name_order(x) for x in xvec])\n",
    "df_for_sorting_expids = df_for_sorting_expids.sort_values('itr')\n",
    "# df_for_sorting_expids = df_for_sorting_expids.sort_values('model_type')\n",
    "df_for_sorting_expids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hist(df, exp_id, bins):\n",
    "    ex_df = df[inst_df_all.full_exp_id == f\"{exp_id}\"]\n",
    "    assert len(ex_df) > 0\n",
    "    hist_data, edges = BokehHistogram.get_np_hist(ex_df.iou, bins=bins)\n",
    "    return edges, hist_data\n",
    "\n",
    "full_exp_ids_ordered = list(inst_df_all.full_exp_id.unique())\n",
    "\n",
    "bins = np.linspace(-0.1, 1, 100)  # BokehHistogram.default_bins\n",
    "\n",
    "\n",
    "ridge_df = pd.DataFrame({f\"{ei}\": get_hist(df=inst_df_all, exp_id=ei, bins=bins)[1] for ei in full_exp_ids_ordered})\n",
    "n_insts = ridge_df.sum(axis=0)\n",
    "ridge_df.loc[:,full_exp_ids_ordered] = ridge_df.loc[:,full_exp_ids_ordered].div(n_insts, axis=1)\n",
    "sum_to_1 = ridge_df.sum(axis=0)\n",
    "\n",
    "ridge_df = pd.concat([ridge_df, ridge_df.tail(1)], axis=0)\n",
    "ridge_df.iloc[-1] = 0.0\n",
    "ridge_df = ridge_df.set_index(bins, drop=True)\n",
    "order = full_exp_ids_ordered\n",
    "print(n_insts)\n",
    "print(sum_to_1)\n",
    "ridge_df = ridge_df.loc[:10, :]\n",
    "\n",
    "ridge_df.iloc[-1] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_name = ridge_df.index\n",
    "nd = 2\n",
    "rolling_ridge_df = (\n",
    "    ridge_df.rolling(nd, center=True)\n",
    "    .median()\n",
    "    .rolling(3 * nd, center=False)\n",
    "    .mean()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "ridge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, DatetimeTickFormatter, BasicTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "# output_notebook()\n",
    "output_file('outfile.html')\n",
    "\n",
    "def ridge(category, data, scale=20):\n",
    "    return list(zip([category] * len(data), scale * data))\n",
    "\n",
    "palette = [cc.rainbow[int(np.mod(i * 9, len(cc.rainbow)))] for i in range(len(order))]\n",
    "\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=ridge_df.index.values))\n",
    "p = figure(\n",
    "    y_range=order,\n",
    "    plot_height=900,\n",
    "    plot_width=900,\n",
    "    toolbar_location=None,\n",
    "    title=\"Histograms of IOUs\",\n",
    ")\n",
    "p.title.text_font_size = \"15pt\"\n",
    "p.xaxis.major_label_text_font_size = \"10pt\"\n",
    "p.yaxis.major_label_text_font_size = \"10pt\"\n",
    "\n",
    "for i, country in enumerate(order):\n",
    "    y = ridge(country, ridge_df[country])\n",
    "    source.add(y, country)\n",
    "    p.patch(\n",
    "        \"x\",\n",
    "        country,\n",
    "        color=palette[i],\n",
    "        alpha=0.25,\n",
    "        line_color=\"black\",\n",
    "        line_alpha=0.5,\n",
    "        source=source,\n",
    "    )\n",
    "\n",
    "p.outline_line_color = None\n",
    "p.background_fill_color = \"#efefef\"\n",
    "\n",
    "p.xaxis.formatter = BasicTickFormatter()  #(days=\"%m/%d\")\n",
    "\n",
    "p.ygrid.grid_line_color = None\n",
    "p.xgrid.grid_line_color = \"#dddddd\"\n",
    "p.xgrid.ticker = p.xaxis.ticker\n",
    "\n",
    "p.axis.minor_tick_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "\n",
    "p.y_range.range_padding = 0.85\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ridge_df[order[4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model_type = 'HEAD_TYPE-None'\n",
    "itr = 16000\n",
    "ref_task = 'pred_masks-segm'\n",
    "ref_exp = f\"{((ref_model_type, itr), ref_task)}\"\n",
    "assert ref_exp in inst_df_all.full_exp_id.unique()\n",
    "ref_df = inst_df_all[inst_df_all['full_exp_id'] == ref_exp]\n",
    "\n",
    "diff_df = inst_df_all.copy()\n",
    "diff_colname = f\"iou_diff\"\n",
    "diff_df[diff_colname] = 0\n",
    "for expid in inst_df_all.full_exp_id.unique():\n",
    "    expid_rows = inst_df_all.full_exp_id == expid\n",
    "    diou = subtract_ious(inst_df_all[expid_rows], ref_df)\n",
    "    diff_df.loc[expid_rows, diff_colname] = diou\n",
    "\n",
    "# t = h.filtered_stacked_hist(inst_df_all, 'iou', 'full_exp_id', log_scale=False, sync_x=True, sync_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "h = BokehHistogram()\n",
    "h.height = 200\n",
    "h.width = 800\n",
    "t = h.filtered_stacked_hist(diff_df, diff_colname, 'full_exp_id', log_scale=False, sync_x=True, sync_y=True, xlim=xl, ylim=yl)\n",
    "show(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expid1 = 'HEAD_TYPE-None'\n",
    "task1 = 'pred_masks-segm'\n",
    "expid2 = 'HEAD_TYPE-custom_MATCH-0'\n",
    "task2 = 'pred_masks-segm'\n",
    "\n",
    "ttl = f\"{expid1}, {task1} - {expid2} ({itr})\"\n",
    "h = BokehHistogram()\n",
    "t = h.hist_hover(inst_df_all, 'iou', log_scale=False, title=ttl)\n",
    "show(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = h.filtered_histotabs(inst_df_all, 'iou', 'full_exp_id', log_scale=False)\n",
    "show(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bokeh_catplot.box(\n",
    "    data=inst_df_all,\n",
    "    cats=['model_type', 'task'],\n",
    "    val='iou',\n",
    "    plot_height=600, plot_width=600\n",
    ")\n",
    "show(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bokeh_catplot.strip(\n",
    "    data=inst_df_all,\n",
    "    cats=['model_type', 'task'],\n",
    "    val='iou',\n",
    "    jitter=True,\n",
    "    plot_height=600, plot_width=600\n",
    ")\n",
    "show(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check that df1 and df2 match only when they should\n",
    "\n",
    "print('ALL TRUE: ', all(inst_df1.image_id == inst_df2.image_id), all(inst_df1.gtid == inst_df2.gtid), all(imcatarea_df1.n_inst == imcatarea_df2.n_inst))\n",
    "print('ALL FALSE: ', all(imcatarea_df1.tp == imcatarea_df2.tp), all(imcatarea_df1.fp == imcatarea_df2.fp), all(imcatarea_df1.fn == imcatarea_df2.fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of x, x1, x2\n",
    "# Avg IOU difference excluding FP, TP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: <metric difference> across categories\n",
    "?pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B: <metric difference> across area ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: <metric difference> per category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: <metric difference>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E: <metric difference>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_1_2_by_imareacat['ious_bygt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gt-by-gt IOU differences\n",
    "\n",
    "# Compute per-instance difference in ious grouped by category/imageid\n",
    "def get_agg_from_cocoeval(evalImgs, params):\n",
    "    N = len(params.imgIds)  # number of images\n",
    "    C = len(params.catIds)\n",
    "    A = len(params.areaRng)\n",
    "\n",
    "    def getimcatareaid(imidx, areaidx, catidx):\n",
    "        return A*N*k + N*j + i\n",
    "\n",
    "    agg = {'ious_bygt': {},\n",
    "           'gtids': {},\n",
    "           'dt_fp': {},\n",
    "           'gt_missed': {}\n",
    "          }\n",
    "    for k, catid in enumerate(params.catIds):\n",
    "        for j, arng in enumerate(params.areaRng[:1]):\n",
    "            for i, imgid in enumerate(params.imgIds):\n",
    "                imcatareaidx = getimcatareaid(i, j, k)\n",
    "#                 print(i, j, k, imcatareaidx)\n",
    "                if evalImgs[imcatareaidx] is not None:\n",
    "                    assert (evalImgs[imcatareaidx]['image_id'], evalImgs[imcatareaidx]['category_id'], evalImgs[imcatareaidx]['aRng']) == (imgid, catid, arng)\n",
    "                    agg['ious_bygt'][(i, j, k)] = np.array(evalImgs[imcatareaidx]['gtMatchIous'][0, :])\n",
    "                    agg['gtids'][(i, j, k)] = np.array(evalImgs[imcatareaidx]['gtIds'])\n",
    "                    agg['dt_fp'][(i, j, k)] = sum(x == 0 for x in evalImgs[imcatareaidx]['dtMatches'][0, :])\n",
    "                    agg['gt_missed'][(i, j, k)] = sum(x == 0 for x in evalImgs[imcatareaidx]['gtMatches'][0, :])\n",
    "                    \n",
    "    return agg\n",
    "\n",
    "\n",
    "cocoeval1 = cocoevals[('HEAD_TYPE-custom_MATCH-1', 16000)]['pred_masks1-segm']\n",
    "cocoeval2 = cocoevals[('HEAD_TYPE-custom_MATCH-1', 16000)]['agg-pred_masks1_pred_masks2-segm']\n",
    "\n",
    "agg_1 = get_agg_from_cocoeval(cocoeval1.evalImgs, cocoeval1.params)\n",
    "agg_2 = get_agg_from_cocoeval(cocoeval2.evalImgs, cocoeval2.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff(agg_a, agg_b):  # agg_a - agg_b\n",
    "    diff_ret = {}\n",
    "    for stat_k in agg_a.keys():\n",
    "        assert set(agg_a[stat_k].keys()) == set(agg_b[stat_k].keys())\n",
    "        diff_ret[stat_k] = {}\n",
    "        for k in agg_a[stat_k].keys():\n",
    "            if stat_k == 'gtids':\n",
    "                assert all(agg_a[stat_k][k] == agg_b[stat_k][k])\n",
    "                diff_ret[stat_k][k] = agg_a[stat_k][k]\n",
    "            else:\n",
    "                diff_ret[stat_k][k] = agg_a[stat_k][k] - agg_b[stat_k][k]\n",
    "    return diff_ret\n",
    "\n",
    "diff_1_2_by_imareacat = compute_diff(agg_1, agg_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show\n",
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "def get_blank_figure(title, **kwargs):\n",
    "    plot_height=kwargs.pop('plot_height', 250)\n",
    "    p = figure(plot_height=plot_height, title=title, **kwargs)\n",
    "    return p\n",
    "\n",
    "\n",
    "def grouped_bar(d, x_super_name, x_sub_name, y_name, title=None, factor_range=None):\n",
    "    x = [(x1, x2) for x1, x2 in zip(d[x_super_name], d[x_sub_name])]\n",
    "    assert len(x) == len(list(set(x))), x\n",
    "    y = d[y_name]\n",
    "    factor_range = factor_range or FactorRange(*sorted(x))\n",
    "    x_sub_unique = list(set([x_[1] for x_ in x]))\n",
    "\n",
    "    if 'colors' not in d:\n",
    "        cmap = Spectral5 if len(x_sub_unique) <= 5 else Turbo256\n",
    "        colors = [cmap[x_sub_unique.index(x_[1])] for x_ in x]\n",
    "    else:\n",
    "        colors = d['colors']\n",
    "    source = ColumnDataSource(data=dict(x=x, y=y, color=colors, x_super=d[x_super_name], x_sub=d[x_sub_name]))\n",
    "\n",
    "    tooltips=[(f\"{(x_super_name, x_sub_name)}\", \"@x\"), (f\"{x_super_name}\", \"@x_super\"), (f\"{x_sub_name}\", \"@x_sub\"), (f\"{y_name}\", \"@y\")]\n",
    "\n",
    "    p = get_blank_figure(x_range=factor_range, title=title or f\"{x_super_name} {y_name} by {x_sub_name}\",\n",
    "               tooltips=tooltips, tools=\"save,pan,wheel_zoom,box_zoom,reset\")  # toolbar_location=None, \n",
    "\n",
    "    p.vbar(x='x', top='y', width=0.9, color='color', source=source)\n",
    "\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds1.ious[(139, 67)]\n",
    "imcatidx = 1000\n",
    "# print(ds1.evalImgs[imcatidx]['gtMatchIous'][0, :])\n",
    "print(ds1.evalImgs[imcatidx]['image_id'], ds1.evalImgs[imcatidx]['category_id'])\n",
    "# print(ds1.ious[(ds1.evalImgs[imcatidx]['image_id'], ds1.evalImgs[imcatidx]['category_id'])]) # DT x GT (confirmed)\n",
    "print('gt', ds1.evalImgs[imcatidx]['gtIds']) # GT x DT\n",
    "print('dt', ds1.evalImgs[imcatidx]['dtIds']) # GT x DT\n",
    "print(ds1.ious[(ds1.evalImgs[imcatidx]['image_id'], ds1.evalImgs[imcatidx]['category_id'])].shape)\n",
    "print(ds1.ious[(ds1.evalImgs[imcatidx]['image_id'], ds1.evalImgs[imcatidx]['category_id'])].max(axis=0)) # one per GT\n",
    "print(ds1.evalImgs[imcatidx].keys()) # one per GT\n",
    "print(ds1.evalImgs[imcatidx]) # one per GT?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
